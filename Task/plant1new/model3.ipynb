{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c7315892",
   "metadata": {},
   "outputs": [],
   "source": [
    "## model\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, Model, Input\n",
    "from xgboost import XGBRegressor\n",
    "import joblib\n",
    "\n",
    "# ============================\n",
    "# 1. Load dataset\n",
    "# ============================\n",
    "df = pd.read_csv(r\"C:\\Users\\User\\Desktop\\solar_code\\Task\\plant1\\Plant1_Merged_Final.csv\")\n",
    "\n",
    "# Ensure datetime format\n",
    "df['DATE_TIME'] = pd.to_datetime(df['DATE_TIME'])\n",
    "\n",
    "# Time features\n",
    "df['hour'] = df['DATE_TIME'].dt.hour\n",
    "df['minute'] = df['DATE_TIME'].dt.minute\n",
    "df['time_fraction'] = df['hour'] + df['minute']/60\n",
    "\n",
    "# Cyclical encoding\n",
    "df['sin_time'] = np.sin(2 * np.pi * df['time_fraction'] / 24)\n",
    "df['cos_time'] = np.cos(2 * np.pi * df['time_fraction'] / 24)\n",
    "\n",
    "# Drop unused columns\n",
    "df = df.drop(columns=['hour','minute','time_fraction','DATE_TIME'])\n",
    "\n",
    "# Features and target\n",
    "X = df.drop(columns=['AC_POWER'])\n",
    "y = df['AC_POWER'].values.reshape(-1,1)   # raw watts\n",
    "\n",
    "# ============================\n",
    "# 2. Create sliding windows\n",
    "# ============================\n",
    "sequence_length = 6\n",
    "X_seq, y_seq = [], []\n",
    "\n",
    "for i in range(len(X) - sequence_length):\n",
    "    X_seq.append(X.iloc[i:i+sequence_length].values)\n",
    "    y_seq.append(y[i+sequence_length])\n",
    "\n",
    "X_seq = np.array(X_seq)\n",
    "y_seq = np.array(y_seq).reshape(-1,1)\n",
    "\n",
    "# ============================\n",
    "# 3. Scale X and y\n",
    "# ============================\n",
    "scaler_X = MinMaxScaler()\n",
    "X_flat = X_seq.reshape(-1, X_seq.shape[-1])\n",
    "X_scaled = scaler_X.fit_transform(X_flat).reshape(X_seq.shape)\n",
    "\n",
    "scaler_y = MinMaxScaler()\n",
    "y_scaled = scaler_y.fit_transform(y_seq)   # scaled target\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bb0db3cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training CNN...\n",
      "Epoch 1/100\n",
      "\u001b[1m163/163\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.0121 - val_loss: 0.0069\n",
      "Epoch 2/100\n",
      "\u001b[1m163/163\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.0081 - val_loss: 0.0063\n",
      "Epoch 3/100\n",
      "\u001b[1m163/163\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.0080 - val_loss: 0.0063\n",
      "Epoch 4/100\n",
      "\u001b[1m163/163\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0077 - val_loss: 0.0063\n",
      "Epoch 5/100\n",
      "\u001b[1m163/163\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0076 - val_loss: 0.0060\n",
      "Epoch 6/100\n",
      "\u001b[1m163/163\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.0075 - val_loss: 0.0064\n",
      "Epoch 7/100\n",
      "\u001b[1m163/163\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.0071 - val_loss: 0.0063\n",
      "Epoch 8/100\n",
      "\u001b[1m163/163\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.0073 - val_loss: 0.0057\n",
      "Epoch 9/100\n",
      "\u001b[1m163/163\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.0073 - val_loss: 0.0060\n",
      "Epoch 10/100\n",
      "\u001b[1m163/163\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.0070 - val_loss: 0.0056\n",
      "Epoch 11/100\n",
      "\u001b[1m163/163\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.0066 - val_loss: 0.0067\n",
      "Epoch 12/100\n",
      "\u001b[1m163/163\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.0065 - val_loss: 0.0055\n",
      "Epoch 13/100\n",
      "\u001b[1m163/163\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.0067 - val_loss: 0.0061\n",
      "Epoch 14/100\n",
      "\u001b[1m163/163\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0066 - val_loss: 0.0054\n",
      "Epoch 15/100\n",
      "\u001b[1m163/163\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.0067 - val_loss: 0.0057\n",
      "Epoch 16/100\n",
      "\u001b[1m163/163\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.0066 - val_loss: 0.0056\n",
      "Epoch 17/100\n",
      "\u001b[1m163/163\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.0063 - val_loss: 0.0065\n",
      "Epoch 18/100\n",
      "\u001b[1m163/163\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.0066 - val_loss: 0.0056\n",
      "Epoch 19/100\n",
      "\u001b[1m163/163\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0066 - val_loss: 0.0057\n",
      "\u001b[1m102/102\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n",
      "Extracted CNN feature shape: (3258, 64)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ============================\n",
    "# 4. CNN Model\n",
    "# ============================\n",
    "timesteps = X_scaled.shape[1]\n",
    "n_features = X_scaled.shape[2]\n",
    "\n",
    "inputs = Input(shape=(timesteps, n_features))\n",
    "x = layers.Conv1D(64, 3, activation='relu', padding=\"causal\")(inputs)\n",
    "x = layers.Conv1D(32, 3, activation='relu', padding=\"causal\")(x)\n",
    "x = layers.GlobalAveragePooling1D()(x)\n",
    "x = layers.Dense(64, activation='relu')(x)\n",
    "outputs = layers.Dense(1)(x)\n",
    "\n",
    "cnn_model = Model(inputs, outputs)\n",
    "cnn_model.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "print(\"\\nTraining CNN...\")\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "es = EarlyStopping(patience=5, restore_best_weights=True)\n",
    "history = cnn_model.fit(\n",
    "    X_scaled, y_scaled,                # ğŸ”¹ target is scaled\n",
    "    epochs=100,\n",
    "    batch_size=16,\n",
    "    validation_split=0.2,\n",
    "    callbacks=[es],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# ============================\n",
    "# 5. Extract CNN features\n",
    "# ============================\n",
    "cnn_feature_extractor = Model(inputs=cnn_model.input, outputs=cnn_model.layers[-2].output)\n",
    "cnn_features = cnn_feature_extractor.predict(X_scaled)\n",
    "print(\"Extracted CNN feature shape:\", cnn_features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8fe96e06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training XGBoost on Plant1 (80% train, 20% test)...\n",
      "ğŸ“Š MSE: 5473416.24\n",
      "ğŸ“Š RMSE: 2339.53\n",
      "ğŸ“Š RÂ²: 0.9233\n",
      "ğŸ“Š MAE: 1154.23\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# ============================\n",
    "# 6. Train/Test split on Plant1\n",
    "# ============================\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    cnn_features, y_scaled, test_size=0.2, random_state=42, shuffle=True\n",
    ")\n",
    "\n",
    "xgb_model = XGBRegressor(\n",
    "    n_estimators=1000,\n",
    "    learning_rate=0.01,\n",
    "    max_depth=6,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    reg_lambda=1.0,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "print(\"\\nTraining XGBoost on Plant1 (80% train, 20% test)...\")\n",
    "xgb_model.fit(X_train, y_train)\n",
    "\n",
    "# ============================\n",
    "# 7. Evaluate (inverse-transform back to raw y)\n",
    "# ============================\n",
    "y_pred_scaled = xgb_model.predict(X_test).reshape(-1,1)\n",
    "y_pred = scaler_y.inverse_transform(y_pred_scaled)\n",
    "y_test_raw = scaler_y.inverse_transform(y_test)\n",
    "\n",
    "mse = mean_squared_error(y_test_raw, y_pred)\n",
    "rmse = np.sqrt(mse)\n",
    "r2 = r2_score(y_test_raw, y_pred)\n",
    "mae = mean_absolute_error(y_test_raw, y_pred)\n",
    "\n",
    "print(f\"ğŸ“Š MSE: {mse:.2f}\")\n",
    "print(f\"ğŸ“Š RMSE: {rmse:.2f}\")\n",
    "print(f\"ğŸ“Š RÂ²: {r2:.4f}\")\n",
    "print(f\"ğŸ“Š MAE: {mae:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ae7faf43",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Models and scalers saved successfully!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ============================\n",
    "# 8. Save models & scalers\n",
    "# ============================\n",
    "joblib.dump(scaler_X, r\"C:\\Users\\User\\Desktop\\solar_code\\Task\\plant1new\\modely\\scaler_X.pkl\")\n",
    "joblib.dump(scaler_y, r\"C:\\Users\\User\\Desktop\\solar_code\\Task\\plant1new\\modely\\scaler_y.pkl\")\n",
    "cnn_model.save(r\"C:\\Users\\User\\Desktop\\solar_code\\Task\\plant1new\\modely\\cnn_model.h5\", include_optimizer=False)\n",
    "xgb_model.save_model(r\"C:\\Users\\User\\Desktop\\solar_code\\Task\\plant1new\\modely\\xgb_model.json\")\n",
    "print(\"âœ… Models and scalers saved successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "983c5b69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m102/102\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n",
      "âœ… Full Plant1 Inference Complete\n",
      "MSE : 1445840.83\n",
      "RMSE: 1202.43\n",
      "MAE : 527.49\n",
      "RÂ²  : 0.9802\n",
      "\n",
      "ğŸ”¹ First 10 Predictions:\n",
      "True: 0.00 W | Predicted: 70.08 W\n",
      "True: 0.00 W | Predicted: 59.78 W\n",
      "True: 0.00 W | Predicted: 57.31 W\n",
      "True: 0.00 W | Predicted: 57.31 W\n",
      "True: 0.00 W | Predicted: 26.86 W\n",
      "True: 0.00 W | Predicted: 26.86 W\n",
      "True: 0.00 W | Predicted: 26.49 W\n",
      "True: 0.00 W | Predicted: 26.49 W\n",
      "True: 0.00 W | Predicted: 26.49 W\n",
      "True: 0.00 W | Predicted: 30.80 W\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step\n",
      "\n",
      "ğŸ”¹ One Window Example\n",
      "True Next AC Power: 0.00 W | Predicted: 70.08 W\n",
      "\u001b[1m2/2\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "\n",
      "ğŸ”¹ First 60 windows â†’ got 60 predictions\n"
     ]
    }
   ],
   "source": [
    "## inference\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "from tensorflow.keras.models import load_model, Model\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "# ============================\n",
    "# Load scalers and models\n",
    "# ============================\n",
    "scaler_X = joblib.load(r\"C:\\Users\\User\\Desktop\\solar_code\\Task\\plant1new\\modely\\scaler_X.pkl\")\n",
    "scaler_y = joblib.load(r\"C:\\Users\\User\\Desktop\\solar_code\\Task\\plant1new\\modely\\scaler_y.pkl\")\n",
    "\n",
    "cnn_model = load_model(r\"C:\\Users\\User\\Desktop\\solar_code\\Task\\plant1new\\modely\\cnn_model.h5\", compile=False)\n",
    "\n",
    "xgb_model = XGBRegressor()\n",
    "xgb_model.load_model(r\"C:\\Users\\User\\Desktop\\solar_code\\Task\\plant1new\\modely\\xgb_model.json\")\n",
    "\n",
    "# Feature extractor from CNN\n",
    "cnn_feature_extractor = Model(inputs=cnn_model.input, outputs=cnn_model.layers[-2].output)\n",
    "\n",
    "# ============================\n",
    "# Preprocessing (CSV â†’ windows)\n",
    "# ============================\n",
    "def preprocess_and_window(df, sequence_length=6):\n",
    "    df['DATE_TIME'] = pd.to_datetime(df['DATE_TIME'])\n",
    "    df['hour'] = df['DATE_TIME'].dt.hour\n",
    "    df['minute'] = df['DATE_TIME'].dt.minute\n",
    "    df['time_fraction'] = df['hour'] + df['minute']/60\n",
    "\n",
    "    # cyclical encoding\n",
    "    df['sin_time'] = np.sin(2*np.pi*df['time_fraction']/24)\n",
    "    df['cos_time'] = np.cos(2*np.pi*df['time_fraction']/24)\n",
    "\n",
    "    # drop unused\n",
    "    df = df.drop(columns=['hour','minute','time_fraction','DATE_TIME'])\n",
    "\n",
    "    X = df.drop(columns=['AC_POWER'])\n",
    "    y = df['AC_POWER'].values\n",
    "\n",
    "    # sliding windows\n",
    "    X_seq, y_seq = [], []\n",
    "    for i in range(len(X)-sequence_length):\n",
    "        X_seq.append(X.iloc[i:i+sequence_length].values)\n",
    "        y_seq.append(y[i+sequence_length])\n",
    "\n",
    "    X_seq = np.array(X_seq)\n",
    "    y_seq = np.array(y_seq)\n",
    "\n",
    "    # scale features\n",
    "    X_flat = X_seq.reshape(-1, X_seq.shape[-1])\n",
    "    X_scaled = scaler_X.transform(X_flat).reshape(X_seq.shape)\n",
    "\n",
    "    return X_scaled, y_seq\n",
    "\n",
    "\n",
    "# ============================\n",
    "# General Prediction Function\n",
    "# ============================\n",
    "def predict_ac_power(test_csv, sequence_length=6, num_windows=None):\n",
    "    \"\"\"\n",
    "    test_csv: path to CSV\n",
    "    sequence_length: sliding window length\n",
    "    num_windows: None â†’ use full CSV, int â†’ use first N windows\n",
    "    \"\"\"\n",
    "    df_test = pd.read_csv(test_csv)\n",
    "    X_scaled, y_true = preprocess_and_window(df_test, sequence_length)\n",
    "\n",
    "    if num_windows is not None:\n",
    "        X_scaled = X_scaled[:num_windows]\n",
    "        y_true = y_true[:num_windows]\n",
    "\n",
    "    # CNN â†’ XGB\n",
    "    cnn_features = cnn_feature_extractor.predict(X_scaled)\n",
    "    y_pred_scaled = xgb_model.predict(cnn_features).reshape(-1,1)\n",
    "\n",
    "    # inverse-transform predictions to raw watts\n",
    "    y_pred = scaler_y.inverse_transform(y_pred_scaled)\n",
    "\n",
    "    return y_true, y_pred.flatten()\n",
    "\n",
    "\n",
    "# ============================\n",
    "if __name__ == \"__main__\":\n",
    "    test_csv = r\"C:\\Users\\User\\Desktop\\solar_code\\Task\\plant1\\Plant1_Merged_Final.csv\"\n",
    "\n",
    "    # 1. Full CSV Prediction\n",
    "    y_true, y_pred = predict_ac_power(test_csv)\n",
    "    mse = mean_squared_error(y_true, y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "\n",
    "    print(\"âœ… Full Plant1 Inference Complete\")\n",
    "    print(f\"MSE : {mse:.2f}\")\n",
    "    print(f\"RMSE: {rmse:.2f}\")\n",
    "    print(f\"MAE : {mae:.2f}\")\n",
    "    print(f\"RÂ²  : {r2:.4f}\")\n",
    "\n",
    "    # Show first 10 predictions\n",
    "    print(\"\\nğŸ”¹ First 10 Predictions:\")\n",
    "    for i in range(10):\n",
    "        print(f\"True: {y_true[i]:.2f} W | Predicted: {y_pred[i]:.2f} W\")\n",
    "\n",
    "    # 2. Predict only first 1 window\n",
    "    y_true_1, y_pred_1 = predict_ac_power(test_csv, num_windows=1)\n",
    "    print(\"\\nğŸ”¹ One Window Example\")\n",
    "    print(f\"True Next AC Power: {y_true_1[0]:.2f} W | Predicted: {y_pred_1[0]:.2f} W\")\n",
    "\n",
    "    # 3. Predict first 60 windows\n",
    "    y_true_60, y_pred_60 = predict_ac_power(test_csv, num_windows=60)\n",
    "    print(f\"\\nğŸ”¹ First 60 windows â†’ got {len(y_true_60)} predictions\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
